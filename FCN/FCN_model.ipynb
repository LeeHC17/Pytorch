{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee780c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from torchsummary import summary as summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3dcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_8s(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN_8s, self).__init__()\n",
    "        #conv1\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size = (3,3), stride = (1,1), padding = 1)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size = (3,3), stride = (1,1), padding = 1)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2), padding = 0)\n",
    "        #conv2\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2), padding = 0)\n",
    "        #conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2), padding = 0)\n",
    "        #conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2,2), stride = (2,2), padding = 0)\n",
    "        #conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=(3,3), stride = (1,1), padding = 1)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=(2,2), stride = (2,2), padding = 0)\n",
    "        #fully conv\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=(1,1), stride = (1,1), padding = 0)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=(1,1), stride = (1,1), padding = 0)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "        self.score1 = nn.Conv2d(4096, 21, kernel_size = (1,1), stride = (1,1), padding = 0)\n",
    "        #upsampling\n",
    "        self.x2upsamp = nn.ConvTranspose2d(21, 21, kernel_size = (4,4), stride = (2,2), padding = 1)\n",
    "        self.x8upsamp = nn.ConvTranspose2d(21, 21, kernel_size = (16,16), stride = (8,8), padding = 4)\n",
    "        #pool3,4 conv\n",
    "        self.pool3conv = nn.Conv2d(256, 21, kernel_size = (1,1))\n",
    "        self.pool4conv = nn.Conv2d(512, 21, kernel_size = (1,1), padding = 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1_1(self.conv1_1(x))\n",
    "        x = self.relu1_2(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.relu2_1(self.conv2_1(x))\n",
    "        x = self.relu2_2(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.relu3_1(self.conv3_1(x))\n",
    "        x = self.relu3_2(self.conv3_2(x))\n",
    "        x = self.relu3_3(self.conv3_3(x))\n",
    "        x = self.pool3(x)\n",
    "        pool3 = x\n",
    "        \n",
    "        x = self.relu4_1(self.conv4_1(x))\n",
    "        x = self.relu4_2(self.conv4_2(x))\n",
    "        x = self.relu4_3(self.conv4_3(x))\n",
    "        x = self.pool4(x)\n",
    "        pool4 = x\n",
    "        \n",
    "        x = self.relu5_1(self.conv5_1(x))\n",
    "        x = self.relu5_2(self.conv5_2(x))\n",
    "        x = self.relu5_3(self.conv5_3(x))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.drop6(self.relu6(self.conv6(x)))\n",
    "        x = self.drop7(self.relu7(self.conv7(x)))\n",
    "        x = self.score1(x)\n",
    "        \n",
    "        x = self.x2upsamp(x)\n",
    "        \n",
    "        pool4 = self.pool4conv(pool4)\n",
    "\n",
    "        x = x + pool4\n",
    "        x = self.x2upsamp(x)\n",
    "        \n",
    "        pool3 = self.pool3conv(pool3)\n",
    "        \n",
    "        x = x + pool3\n",
    "        x = self.x8upsamp(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43cf910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [6, 64, 256, 256]           1,792\n",
      "              ReLU-2          [6, 64, 256, 256]               0\n",
      "            Conv2d-3          [6, 64, 256, 256]          36,928\n",
      "              ReLU-4          [6, 64, 256, 256]               0\n",
      "         MaxPool2d-5          [6, 64, 128, 128]               0\n",
      "            Conv2d-6         [6, 128, 128, 128]          73,856\n",
      "              ReLU-7         [6, 128, 128, 128]               0\n",
      "            Conv2d-8         [6, 128, 128, 128]         147,584\n",
      "              ReLU-9         [6, 128, 128, 128]               0\n",
      "        MaxPool2d-10           [6, 128, 64, 64]               0\n",
      "           Conv2d-11           [6, 256, 64, 64]         295,168\n",
      "             ReLU-12           [6, 256, 64, 64]               0\n",
      "           Conv2d-13           [6, 256, 64, 64]         590,080\n",
      "             ReLU-14           [6, 256, 64, 64]               0\n",
      "           Conv2d-15           [6, 256, 64, 64]         590,080\n",
      "             ReLU-16           [6, 256, 64, 64]               0\n",
      "        MaxPool2d-17           [6, 256, 32, 32]               0\n",
      "           Conv2d-18           [6, 512, 32, 32]       1,180,160\n",
      "             ReLU-19           [6, 512, 32, 32]               0\n",
      "           Conv2d-20           [6, 512, 32, 32]       2,359,808\n",
      "             ReLU-21           [6, 512, 32, 32]               0\n",
      "           Conv2d-22           [6, 512, 32, 32]       2,359,808\n",
      "             ReLU-23           [6, 512, 32, 32]               0\n",
      "        MaxPool2d-24           [6, 512, 16, 16]               0\n",
      "           Conv2d-25           [6, 512, 16, 16]       2,359,808\n",
      "             ReLU-26           [6, 512, 16, 16]               0\n",
      "           Conv2d-27           [6, 512, 16, 16]       2,359,808\n",
      "             ReLU-28           [6, 512, 16, 16]               0\n",
      "           Conv2d-29           [6, 512, 16, 16]       2,359,808\n",
      "             ReLU-30           [6, 512, 16, 16]               0\n",
      "        MaxPool2d-31             [6, 512, 8, 8]               0\n",
      "           Conv2d-32            [6, 4096, 8, 8]       2,101,248\n",
      "             ReLU-33            [6, 4096, 8, 8]               0\n",
      "        Dropout2d-34            [6, 4096, 8, 8]               0\n",
      "           Conv2d-35            [6, 4096, 8, 8]      16,781,312\n",
      "             ReLU-36            [6, 4096, 8, 8]               0\n",
      "        Dropout2d-37            [6, 4096, 8, 8]               0\n",
      "           Conv2d-38              [6, 21, 8, 8]          86,037\n",
      "  ConvTranspose2d-39            [6, 21, 16, 16]           7,077\n",
      "           Conv2d-40            [6, 21, 16, 16]          10,773\n",
      "  ConvTranspose2d-41            [6, 21, 32, 32]           7,077\n",
      "           Conv2d-42            [6, 21, 32, 32]           5,397\n",
      "  ConvTranspose2d-43          [6, 21, 256, 256]         112,917\n",
      "================================================================\n",
      "Total params: 33,826,526\n",
      "Trainable params: 33,826,526\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.50\n",
      "Forward/backward pass size (MB): 1849.02\n",
      "Params size (MB): 129.04\n",
      "Estimated Total Size (MB): 1982.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_test = FCN_8s()\n",
    "summary_(model_test.to('cuda'), (3, 256, 256), batch_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9229d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_voc_images(train = True):\n",
    "    file = 'VOCdevkit/VOC2012/ImageSets/Segmentation/' + ('train.txt' if train else 'val.txt')\n",
    "    mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    with open(file, 'r') as f:\n",
    "        imgs = f.read().split()\n",
    "    datas, targets = [], []\n",
    "    for i, imgname in enumerate(imgs):\n",
    "        datas.append(torchvision.io.read_image(os.path.join('VOCdevkit/VOC2012/JPEGImages', f'{imgname}.jpg')))\n",
    "        targets.append(torchvision.io.read_image(os.path.join('VOCdevkit/VOC2012/SegmentationClass', f'{imgname}.png'), mode))\n",
    "    return datas, targets\n",
    "\n",
    "Color_Map = [\n",
    "               [0, 0, 0],  # background\n",
    "               [128, 0, 0], # aeroplane\n",
    "               [0, 128, 0], # bicycle\n",
    "               [128, 128, 0], # bird\n",
    "               [0, 0, 128], # boat\n",
    "               [128, 0, 128], # bottle\n",
    "               [0, 128, 128], # bus \n",
    "               [128, 128, 128], # car\n",
    "               [64, 0, 0], # cat\n",
    "               [192, 0, 0], # chair\n",
    "               [64, 128, 0], # cow\n",
    "               [192, 128, 0], # dining table\n",
    "               [64, 0, 128], # dog\n",
    "               [192, 0, 128], # horse\n",
    "               [64, 128, 128], # motorbike\n",
    "               [192, 128, 128], # person\n",
    "               [0, 64, 0], # potted plant\n",
    "               [128, 64, 0], # sheep\n",
    "               [0, 192, 0], # sofa\n",
    "               [128, 192, 0], # train\n",
    "               [0, 64, 128] # tv/monitor\n",
    "]\n",
    "\n",
    "def voccolormap2label():\n",
    "    colormap2label = torch.zeros(256**3, dtype=torch.long)\n",
    "    for i, cm in enumerate(Color_Map):\n",
    "        colormap2label[(cm[0] * 256 + cm[1]) * 256 +cm[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "def voclabel_indices(colormap, colormap2label):\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:,:,0] * 256 + colormap[:,:,1]) * 256 + colormap[:,:,2])\n",
    "    return colormap2label[idx]\n",
    "\n",
    "def vocrand_crop(data, target, h, w):\n",
    "    rect = transforms.RandomCrop.get_params(data, (h,w))\n",
    "    data = transforms.functional.crop(data, *rect)\n",
    "    target = transforms.functional.crop(target, *rect)\n",
    "    return data, target\n",
    "\n",
    "class VOCSegDataset(data.Dataset):\n",
    "    def __init__(self, train, img_size):\n",
    "        self.transform = transforms.Normalize(mean = [0.485,0.456,0.406],\n",
    "                                             std = [0.229,0.224,0.225])\n",
    "        self.img_size = img_size\n",
    "        datas, targets = read_voc_images(train = train)\n",
    "        self.datas = [self.normalize_image(data) for data in self.filter(datas)]\n",
    "        self.targets = self.filter(targets)\n",
    "        self.colormap2label = voccolormap2label()\n",
    "        \n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float())\n",
    "    \n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if(img.shape[1] >= self.img_size[0] and img.shape[2] >= self.img_size[1])]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, target = vocrand_crop(self.datas[idx], self.targets[idx], *self.img_size)\n",
    "        return (data, voclabel_indices(target, self.colormap2label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "###\n",
    "\n",
    "img_size = (256, 256)\n",
    "train_data = VOCSegDataset(train = True, img_size = img_size)\n",
    "valid_data = VOCSegDataset(train = False, img_size = img_size)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=6, shuffle=True)\n",
    "valid_loader = data.DataLoader(valid_data, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b797fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1444 (0%)\tloss:3.0534]\n",
      "Train Epoch: 1 [1200/1444 (83%)\tloss:2.5538]\n",
      "Epoch: [1] val_Loss: 1.9194 val_Accuracy: 63.94%\n",
      "Train Epoch: 2 [0/1444 (0%)\tloss:1.5451]\n",
      "Train Epoch: 2 [1200/1444 (83%)\tloss:1.6758]\n",
      "Epoch: [2] val_Loss: 1.8645 val_Accuracy: 64.37%\n",
      "Train Epoch: 3 [0/1444 (0%)\tloss:1.5923]\n",
      "Train Epoch: 3 [1200/1444 (83%)\tloss:1.7724]\n",
      "Epoch: [3] val_Loss: 1.8197 val_Accuracy: 64.00%\n",
      "Train Epoch: 4 [0/1444 (0%)\tloss:1.5968]\n",
      "Train Epoch: 4 [1200/1444 (83%)\tloss:1.4122]\n",
      "Epoch: [4] val_Loss: 1.7981 val_Accuracy: 64.18%\n",
      "Train Epoch: 5 [0/1444 (0%)\tloss:1.7215]\n",
      "Train Epoch: 5 [1200/1444 (83%)\tloss:1.4503]\n",
      "Epoch: [5] val_Loss: 1.8063 val_Accuracy: 64.09%\n",
      "Train Epoch: 6 [0/1444 (0%)\tloss:2.1909]\n",
      "Train Epoch: 6 [1200/1444 (83%)\tloss:1.6597]\n",
      "Epoch: [6] val_Loss: 1.7914 val_Accuracy: 64.11%\n",
      "Train Epoch: 7 [0/1444 (0%)\tloss:1.2776]\n",
      "Train Epoch: 7 [1200/1444 (83%)\tloss:1.2180]\n",
      "Epoch: [7] val_Loss: 1.7920 val_Accuracy: 64.24%\n",
      "Train Epoch: 8 [0/1444 (0%)\tloss:1.1711]\n",
      "Train Epoch: 8 [1200/1444 (83%)\tloss:2.3292]\n",
      "Epoch: [8] val_Loss: 1.7719 val_Accuracy: 64.35%\n",
      "Train Epoch: 9 [0/1444 (0%)\tloss:2.0317]\n",
      "Train Epoch: 9 [1200/1444 (83%)\tloss:1.6335]\n",
      "Epoch: [9] val_Loss: 1.7736 val_Accuracy: 64.24%\n",
      "Train Epoch: 10 [0/1444 (0%)\tloss:1.8811]\n",
      "Train Epoch: 10 [1200/1444 (83%)\tloss:1.5125]\n",
      "Epoch: [10] val_Loss: 1.7468 val_Accuracy: 64.67%\n",
      "Train Epoch: 11 [0/1444 (0%)\tloss:1.9515]\n",
      "Train Epoch: 11 [1200/1444 (83%)\tloss:1.6023]\n",
      "Epoch: [11] val_Loss: 1.7477 val_Accuracy: 64.38%\n",
      "Train Epoch: 12 [0/1444 (0%)\tloss:1.8796]\n",
      "Train Epoch: 12 [1200/1444 (83%)\tloss:1.8794]\n",
      "Epoch: [12] val_Loss: 1.7566 val_Accuracy: 64.12%\n",
      "Train Epoch: 13 [0/1444 (0%)\tloss:1.3392]\n",
      "Train Epoch: 13 [1200/1444 (83%)\tloss:1.7181]\n",
      "Epoch: [13] val_Loss: 1.7686 val_Accuracy: 63.61%\n",
      "Train Epoch: 14 [0/1444 (0%)\tloss:2.4541]\n",
      "Train Epoch: 14 [1200/1444 (83%)\tloss:2.1539]\n",
      "Epoch: [14] val_Loss: 1.7372 val_Accuracy: 64.29%\n",
      "Train Epoch: 15 [0/1444 (0%)\tloss:1.5060]\n",
      "Train Epoch: 15 [1200/1444 (83%)\tloss:1.5192]\n",
      "Epoch: [15] val_Loss: 1.7559 val_Accuracy: 64.37%\n",
      "Train Epoch: 16 [0/1444 (0%)\tloss:1.9152]\n",
      "Train Epoch: 16 [1200/1444 (83%)\tloss:1.9282]\n",
      "Epoch: [16] val_Loss: 1.7446 val_Accuracy: 64.43%\n",
      "Train Epoch: 17 [0/1444 (0%)\tloss:1.5755]\n",
      "Train Epoch: 17 [1200/1444 (83%)\tloss:1.4050]\n",
      "Epoch: [17] val_Loss: 1.7336 val_Accuracy: 64.37%\n",
      "Train Epoch: 18 [0/1444 (0%)\tloss:1.4030]\n",
      "Train Epoch: 18 [1200/1444 (83%)\tloss:1.9401]\n",
      "Epoch: [18] val_Loss: 1.7446 val_Accuracy: 64.38%\n",
      "Train Epoch: 19 [0/1444 (0%)\tloss:1.4242]\n",
      "Train Epoch: 19 [1200/1444 (83%)\tloss:1.6806]\n",
      "Epoch: [19] val_Loss: 1.7332 val_Accuracy: 64.48%\n",
      "Train Epoch: 20 [0/1444 (0%)\tloss:1.3189]\n",
      "Train Epoch: 20 [1200/1444 (83%)\tloss:1.3775]\n",
      "Epoch: [20] val_Loss: 1.7158 val_Accuracy: 64.43%\n",
      "Train Epoch: 21 [0/1444 (0%)\tloss:1.8683]\n",
      "Train Epoch: 21 [1200/1444 (83%)\tloss:1.2349]\n",
      "Epoch: [21] val_Loss: 1.7409 val_Accuracy: 64.05%\n",
      "Train Epoch: 22 [0/1444 (0%)\tloss:1.3144]\n",
      "Train Epoch: 22 [1200/1444 (83%)\tloss:1.6054]\n",
      "Epoch: [22] val_Loss: 1.7213 val_Accuracy: 64.43%\n",
      "Train Epoch: 23 [0/1444 (0%)\tloss:1.4737]\n",
      "Train Epoch: 23 [1200/1444 (83%)\tloss:1.7334]\n",
      "Epoch: [23] val_Loss: 1.7445 val_Accuracy: 64.31%\n",
      "Train Epoch: 24 [0/1444 (0%)\tloss:1.7706]\n",
      "Train Epoch: 24 [1200/1444 (83%)\tloss:1.0712]\n",
      "Epoch: [24] val_Loss: 1.7302 val_Accuracy: 64.04%\n",
      "Train Epoch: 25 [0/1444 (0%)\tloss:1.9021]\n",
      "Train Epoch: 25 [1200/1444 (83%)\tloss:1.7929]\n",
      "Epoch: [25] val_Loss: 1.7221 val_Accuracy: 64.17%\n",
      "Train Epoch: 26 [0/1444 (0%)\tloss:1.6780]\n",
      "Train Epoch: 26 [1200/1444 (83%)\tloss:1.7534]\n",
      "Epoch: [26] val_Loss: 1.7183 val_Accuracy: 64.14%\n",
      "Train Epoch: 27 [0/1444 (0%)\tloss:2.2115]\n",
      "Train Epoch: 27 [1200/1444 (83%)\tloss:1.6231]\n",
      "Epoch: [27] val_Loss: 1.7171 val_Accuracy: 64.16%\n",
      "Train Epoch: 28 [0/1444 (0%)\tloss:1.6303]\n",
      "Train Epoch: 28 [1200/1444 (83%)\tloss:0.8723]\n",
      "Epoch: [28] val_Loss: 1.7187 val_Accuracy: 63.99%\n",
      "Train Epoch: 29 [0/1444 (0%)\tloss:1.3092]\n",
      "Train Epoch: 29 [1200/1444 (83%)\tloss:1.6494]\n",
      "Epoch: [29] val_Loss: 1.7321 val_Accuracy: 64.15%\n",
      "Train Epoch: 30 [0/1444 (0%)\tloss:1.5033]\n",
      "Train Epoch: 30 [1200/1444 (83%)\tloss:1.4151]\n",
      "Epoch: [30] val_Loss: 1.7039 val_Accuracy: 64.18%\n",
      "Train Epoch: 31 [0/1444 (0%)\tloss:1.9023]\n",
      "Train Epoch: 31 [1200/1444 (83%)\tloss:1.5914]\n",
      "Epoch: [31] val_Loss: 1.7151 val_Accuracy: 64.04%\n",
      "Train Epoch: 32 [0/1444 (0%)\tloss:1.6390]\n",
      "Train Epoch: 32 [1200/1444 (83%)\tloss:2.0310]\n",
      "Epoch: [32] val_Loss: 1.7163 val_Accuracy: 63.80%\n",
      "Train Epoch: 33 [0/1444 (0%)\tloss:1.1071]\n",
      "Train Epoch: 33 [1200/1444 (83%)\tloss:1.3168]\n",
      "Epoch: [33] val_Loss: 1.7109 val_Accuracy: 64.02%\n",
      "Train Epoch: 34 [0/1444 (0%)\tloss:1.6381]\n",
      "Train Epoch: 34 [1200/1444 (83%)\tloss:1.6667]\n",
      "Epoch: [34] val_Loss: 1.6801 val_Accuracy: 64.90%\n",
      "Train Epoch: 35 [0/1444 (0%)\tloss:1.4632]\n",
      "Train Epoch: 35 [1200/1444 (83%)\tloss:1.7288]\n",
      "Epoch: [35] val_Loss: 1.7026 val_Accuracy: 64.38%\n",
      "Train Epoch: 36 [0/1444 (0%)\tloss:1.7528]\n",
      "Train Epoch: 36 [1200/1444 (83%)\tloss:2.1800]\n",
      "Epoch: [36] val_Loss: 1.7066 val_Accuracy: 63.99%\n",
      "Train Epoch: 37 [0/1444 (0%)\tloss:1.7014]\n",
      "Train Epoch: 37 [1200/1444 (83%)\tloss:1.6649]\n",
      "Epoch: [37] val_Loss: 1.7068 val_Accuracy: 64.30%\n",
      "Train Epoch: 38 [0/1444 (0%)\tloss:1.6006]\n",
      "Train Epoch: 38 [1200/1444 (83%)\tloss:1.5480]\n",
      "Epoch: [38] val_Loss: 1.7003 val_Accuracy: 64.25%\n",
      "Train Epoch: 39 [0/1444 (0%)\tloss:2.0497]\n",
      "Train Epoch: 39 [1200/1444 (83%)\tloss:1.7002]\n",
      "Epoch: [39] val_Loss: 1.6997 val_Accuracy: 64.48%\n",
      "Train Epoch: 40 [0/1444 (0%)\tloss:1.6531]\n",
      "Train Epoch: 40 [1200/1444 (83%)\tloss:1.4056]\n",
      "Epoch: [40] val_Loss: 1.7049 val_Accuracy: 64.19%\n",
      "Train Epoch: 41 [0/1444 (0%)\tloss:2.2034]\n",
      "Train Epoch: 41 [1200/1444 (83%)\tloss:2.1238]\n",
      "Epoch: [41] val_Loss: 1.6831 val_Accuracy: 64.36%\n",
      "Train Epoch: 42 [0/1444 (0%)\tloss:1.8994]\n",
      "Train Epoch: 42 [1200/1444 (83%)\tloss:1.8247]\n",
      "Epoch: [42] val_Loss: 1.6980 val_Accuracy: 63.99%\n",
      "Train Epoch: 43 [0/1444 (0%)\tloss:1.2868]\n",
      "Train Epoch: 43 [1200/1444 (83%)\tloss:1.7106]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9d21fe1e1181>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_criterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model_{}.pth'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9d21fe1e1181>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(device, model, valid_loader, criterion, epoch)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-13397ec1c42f>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocrand_crop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoclabel_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolormap2label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-13397ec1c42f>\u001b[0m in \u001b[0;36mvoclabel_indices\u001b[1;34m(colormap, colormap2label)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvoclabel_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap2label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mcolormap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcolormap2label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = FCN_8s()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "test_criterion = nn.NLLLoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001, weight_decay = 0.0016, momentum = 0.9)\n",
    "model_dir = 'D:/현장 실습/8주차'\n",
    "\n",
    "def train(device, mode, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        inputs = Variable(data)\n",
    "        targets = Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)\\tloss:{:.4f}]'\n",
    "                 .format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100 * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "\n",
    "            \n",
    "def evaluate(device, model, valid_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    #count = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(data, target) in enumerate(valid_loader):\n",
    "            N = data.size(0)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            inputs = Variable(data)\n",
    "            targets = Variable(target) \n",
    "            \n",
    "            output = model(inputs)\n",
    "            \n",
    "            test_loss += criterion(F.log_softmax(output, dim=1), targets).item()\n",
    "            #count += N\n",
    "            #loss = criterion(F.log_softmax(output, dim=1),targets)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += targets.nelement()\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "    \n",
    "    test_loss_avg = test_loss / total\n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    print('Epoch: [{}] val_Loss: {:.4f} val_Accuracy: {:.2f}%'\n",
    "          .format(epoch, test_loss_avg, test_accuracy))\n",
    "    \n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(device, model, train_loader, optimizer, criterion, epoch)\n",
    "    evaluate(device, model, valid_loader, test_criterion, epoch)\n",
    "    if (epoch > 1 and epoch%10 == 0):\n",
    "        torch.save(model.state_dict(), 'model_{}.pth'.format(epoch))\n",
    "        torch.save(model, model_dir + 'all_model_{}.pth'.format(epoch))\n",
    "    #print('[{}] Test Loss: {:.4f}, Accuracy: {:.0f}%'.format(epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FCN_8s()\n",
    "model2.load_state_dict(torch.load('model_50.pth'))\n",
    "\n",
    "for epoch in range(51, 151):\n",
    "    train(device, model2, train_loader, optimizer, criterion, epoch)\n",
    "    evaluate(device, model2, valid_loader, test_criterion, epoch)d\n",
    "    if(epoch==80 or epoch==100 or epoch==120 or epoch==150):\n",
    "        torch.save(model.state_dict(), 'model_{}.pth'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
